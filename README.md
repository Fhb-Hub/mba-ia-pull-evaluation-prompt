# Pull, Otimiza√ß√£o e Avalia√ß√£o de Prompts com LangChain e LangSmith

## Objetivo

Voc√™ deve entregar um software capaz de:

1. **Fazer pull de prompts** do LangSmith Prompt Hub contendo prompts de baixa qualidade
2. **Refatorar e otimizar** esses prompts usando t√©cnicas avan√ßadas de Prompt Engineering
3. **Fazer push dos prompts otimizados** de volta ao LangSmith
4. **Avaliar a qualidade** atrav√©s de m√©tricas customizadas (F1-Score, Clarity, Precision)
5. **Atingir pontua√ß√£o m√≠nima** de 0.9 (90%) em todas as m√©tricas de avalia√ß√£o

---

## Exemplo no CLI

```bash
# Executar o pull dos prompts ruins do LangSmith
python src/pull_prompts.py

# Executar avalia√ß√£o inicial (prompts ruins)
python src/evaluate.py

Executando avalia√ß√£o dos prompts...
================================
Prompt: support_bot_v1a
- Helpfulness: 0.45
- Correctness: 0.52
- F1-Score: 0.48
- Clarity: 0.50
- Precision: 0.46
================================
Status: FALHOU - M√©tricas abaixo do m√≠nimo de 0.9

# Ap√≥s refatorar os prompts e fazer push
python src/push_prompts.py

# Executar avalia√ß√£o final (prompts otimizados)
python src/evaluate.py

Executando avalia√ß√£o dos prompts...
================================
Prompt: support_bot_v2_optimized
- Helpfulness: 0.94
- Correctness: 0.96
- F1-Score: 0.93
- Clarity: 0.95
- Precision: 0.92
================================
Status: APROVADO ‚úì - Todas as m√©tricas atingiram o m√≠nimo de 0.9
```
---

## Tecnologias obrigat√≥rias

- **Linguagem:** Python 3.9+
- **Framework:** LangChain
- **Plataforma de avalia√ß√£o:** LangSmith
- **Gest√£o de prompts:** LangSmith Prompt Hub
- **Formato de prompts:** YAML

---

## Pacotes recomendados

```python
from langchain import hub  # Pull e Push de prompts
from langsmith import Client  # Intera√ß√£o com LangSmith API
from langsmith.evaluation import evaluate  # Avalia√ß√£o de prompts
from langchain_openai import ChatOpenAI  # LLM OpenAI
from langchain_google_genai import ChatGoogleGenerativeAI  # LLM Gemini
```

---

## OpenAI

- Crie uma **API Key** da OpenAI: https://platform.openai.com/api-keys
- **Modelo de LLM para responder**: `gpt-4o-mini`
- **Modelo de LLM para avalia√ß√£o**: `gpt-4o`
- **Custo estimado:** ~$1-5 para completar o desafio

## Gemini (modelo free)

- Crie uma **API Key** da Google: https://aistudio.google.com/app/apikey
- **Modelo de LLM para responder**: `gemini-2.5-flash`
- **Modelo de LLM para avalia√ß√£o**: `gemini-2.5-flash`
- **Limite:** 15 req/min, 1500 req/dia

---

## Requisitos

### 1. Pull dos Prompt inicial do LangSmith

O reposit√≥rio base j√° cont√©m prompts de **baixa qualidade** publicados no LangSmith Prompt Hub. Sua primeira tarefa √© criar o c√≥digo capaz de fazer o pull desses prompts para o seu ambiente local.

**Tarefas:**

1. Configurar suas credenciais do LangSmith no arquivo `.env` (conforme instru√ß√µes no `README.md` do reposit√≥rio base)
2. Acessar o script `src/pull_prompts.py` que:
   - Conecta ao LangSmith usando suas credenciais
   - Faz pull do seguinte prompts:
     - `leonanluppi/bug_to_user_story_v1`
   - Salva os prompts localmente em `prompts/bug_to_user_story_v1.yml`

---

### 2. Otimiza√ß√£o do Prompt

Agora que voc√™ tem o prompt inicial, √© hora de refator√°-lo usando as t√©cnicas de prompt aprendidas no curso.

**Tarefas:**

1. Analisar o prompt em `prompts/bug_to_user_story_v1.yml`
2. Criar um novo arquivo `prompts/bug_to_user_story_v2.yml` com suas vers√µes otimizadas
3. Aplicar **pelo menos duas** das seguintes t√©cnicas:
   - **Few-shot Learning**: Fornecer exemplos claros de entrada/sa√≠da
   - **Chain of Thought (CoT)**: Instruir o modelo a "pensar passo a passo"
   - **Tree of Thought**: Explorar m√∫ltiplos caminhos de racioc√≠nio
   - **Skeleton of Thought**: Estruturar a resposta em etapas claras
   - **ReAct**: Racioc√≠nio + A√ß√£o para tarefas complexas
   - **Role Prompting**: Definir persona e contexto detalhado
4. Documentar no `README.md` quais t√©cnicas voc√™ escolheu e por qu√™

**Requisitos do prompt otimizado:**

- Deve conter **instru√ß√µes claras e espec√≠ficas**
- Deve incluir **regras expl√≠citas** de comportamento
- Deve ter **exemplos de entrada/sa√≠da** (Few-shot)
- Deve incluir **tratamento de edge cases**
- Deve usar **System vs User Prompt** adequadamente

---

### 3. Push e Avalia√ß√£o

Ap√≥s refatorar os prompts, voc√™ deve envi√°-los de volta ao LangSmith Prompt Hub.

**Tarefas:**

1. Criar o script `src/push_prompts.py` que:
   - L√™ os prompts otimizados de `prompts/bug_to_user_story_v2.yml`
   - Faz push para o LangSmith com nomes versionados:
     - `{seu_username}/bug_to_user_story_v2`
   - Adiciona metadados (tags, descri√ß√£o, t√©cnicas utilizadas)
2. Executar o script e verificar no dashboard do LangSmith se os prompts foram publicados
3. Deixa-lo p√∫blico

---

### 4. Itera√ß√£o

- Espera-se 3-5 itera√ß√µes.
- Analisar m√©tricas baixas e identificar problemas
- Editar prompt, fazer push e avaliar novamente
- Repetir at√© **TODAS as m√©tricas >= 0.9**

### Crit√©rio de Aprova√ß√£o:

```
- Tone Score >= 0.9
- Acceptance Criteria Score >= 0.9
- User Story Format Score >= 0.9
- Completeness Score >= 0.9

M√âDIA das 4 m√©tricas >= 0.9
```

**IMPORTANTE:** TODAS as 4 m√©tricas devem estar >= 0.9, n√£o apenas a m√©dia!

### 5. Testes de Valida√ß√£o

**O que voc√™ deve fazer:** Edite o arquivo `tests/test_prompts.py` e implemente, no m√≠nimo, os 6 testes abaixo usando `pytest`:

- `test_prompt_has_system_prompt`: Verifica se o campo existe e n√£o est√° vazio.
- `test_prompt_has_role_definition`: Verifica se o prompt define uma persona (ex: "Voc√™ √© um Product Manager").
- `test_prompt_mentions_format`: Verifica se o prompt exige formato Markdown ou User Story padr√£o.
- `test_prompt_has_few_shot_examples`: Verifica se o prompt cont√©m exemplos de entrada/sa√≠da (t√©cnica Few-shot).
- `test_prompt_no_todos`: Garante que voc√™ n√£o esqueceu nenhum `[TODO]` no texto.
- `test_minimum_techniques`: Verifica (atrav√©s dos metadados do yaml) se pelo menos 2 t√©cnicas foram listadas.

**Como validar:**

```bash
pytest tests/test_prompts.py
```

---

## Estrutura obrigat√≥ria do projeto

Fa√ßa um fork do reposit√≥rio base: **[Clique aqui para o template](https://github.com/devfullcycle/mba-ia-pull-evaluation-prompt)**

```
desafio-prompt-engineer/
‚îú‚îÄ‚îÄ .env.example              # Template das vari√°veis de ambiente
‚îú‚îÄ‚îÄ requirements.txt          # Depend√™ncias Python
‚îú‚îÄ‚îÄ README.md                 # Sua documenta√ß√£o do processo
‚îÇ
‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îú‚îÄ‚îÄ bug_to_user_story_v1.yml       # Prompt inicial (ap√≥s pull)
‚îÇ   ‚îî‚îÄ‚îÄ bug_to_user_story_v2.yml # Seu prompt otimizado
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ pull_prompts.py       # Pull do LangSmith
‚îÇ   ‚îú‚îÄ‚îÄ push_prompts.py       # Push ao LangSmith
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py           # Avalia√ß√£o autom√°tica
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py            # 4 m√©tricas implementadas
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py            # 15 exemplos de bugs
‚îÇ   ‚îî‚îÄ‚îÄ utils.py              # Fun√ß√µes auxiliares
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_prompts.py       # Testes de valida√ß√£o
‚îÇ
```

**O que voc√™ vai criar:**

- `prompts/bug_to_user_story_v2.yml` - Seu prompt otimizado
- `tests/test_prompts.py` - Seus testes de valida√ß√£o
- `src/pull_prompt.py` Script de pull do reposit√≥rio da fullcycle
- `src/push_prompt.py` Script de push para o seu reposit√≥rio
- `README.md` - Documenta√ß√£o do seu processo de otimiza√ß√£o

**O que j√° vem pronto:**

- Dataset com 15 bugs (5 simples, 7 m√©dios, 3 complexos)
- 4 m√©tricas espec√≠ficas para Bug to User Story
- Suporte multi-provider (OpenAI e Gemini)

## Reposit√≥rios √∫teis

- [Reposit√≥rio boilerplate do desafio](https://github.com/devfullcycle/mba-ia-pull-evaluation-prompt)
- [LangSmith Documentation](https://docs.smith.langchain.com/)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)

## VirtualEnv para Python

Crie e ative um ambiente virtual antes de instalar depend√™ncias:

```bash
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
pip install -r requirements.txt
```

---

## Ordem de execu√ß√£o

### 1. Executar pull dos prompts ruins

```bash
python src/pull_prompts.py
```

### 2. Refatorar prompts

Edite manualmente o arquivo `prompts/bug_to_user_story_v2.yml` aplicando as t√©cnicas aprendidas no curso.

### 3. Fazer push dos prompts otimizados

```bash
python src/push_prompts.py
```

### 5. Executar avalia√ß√£o

```bash
python src/evaluate.py
```

---

## Entreg√°vel

1. **Reposit√≥rio p√∫blico no GitHub** (fork do reposit√≥rio base) contendo:

   - Todo o c√≥digo-fonte implementado
   - Arquivo `prompts/bug_to_user_story_v2.yml` 100% preenchido e funcional
   - Arquivo `README.md` atualizado com:

2. **README.md deve conter:**

   A) **Se√ß√£o "T√©cnicas Aplicadas (Fase 2)"**:

   - Quais t√©cnicas avan√ßadas voc√™ escolheu para refatorar os prompts
   - Justificativa de por que escolheu cada t√©cnica
   - Exemplos pr√°ticos de como aplicou cada t√©cnica

   B) **Se√ß√£o "Resultados Finais"**:

   - Link p√∫blico do seu dashboard do LangSmith mostrando as avalia√ß√µes
   - Screenshots das avalia√ß√µes com as notas m√≠nimas de 0.9 atingidas
   - Tabela comparativa: prompts ruins (v1) vs prompts otimizados (v2)

   C) **Se√ß√£o "Como Executar"**:

   - Instru√ß√µes claras e detalhadas de como executar o projeto
   - Pr√©-requisitos e depend√™ncias
   - Comandos para cada fase do projeto

3. **Evid√™ncias no LangSmith**:
   - Link p√∫blico (ou screenshots) do dashboard do LangSmith
   - Devem estar vis√≠veis:

     - Dataset de avalia√ß√£o com ‚â• 20 exemplos
     - Execu√ß√µes dos prompts v1 (ruins) com notas baixas
     - Execu√ß√µes dos prompts v2 (otimizados) com notas ‚â• 0.9
     - Tracing detalhado de pelo menos 3 exemplos

---

## Dicas Finais

- **Lembre-se da import√¢ncia da especificidade, contexto e persona** ao refatorar prompts
- **Use Few-shot Learning com 2-3 exemplos claros** para melhorar drasticamente a performance
- **Chain of Thought (CoT)** √© excelente para tarefas que exigem racioc√≠nio complexo (como an√°lise de PRs)
- **Use o Tracing do LangSmith** como sua principal ferramenta de debug - ele mostra exatamente o que o LLM est√° "pensando"
- **N√£o altere os datasets de avalia√ß√£o** - apenas os prompts em `prompts/bug_to_user_story_v2.yml`
- **Itere, itere, itere** - √© normal precisar de 3-5 itera√ß√µes para atingir 0.9 em todas as m√©tricas
- **Documente seu processo** - a jornada de otimiza√ß√£o √© t√£o importante quanto o resultado final

---

## T√©cnicas Aplicadas (Fase 2)

### 1. Role Prompting
**Justificativa:** Define uma persona clara (Product Manager s√™nior) para dar contexto ao modelo sobre o tipo de resposta esperada e o n√≠vel de expertise desejado.

**Aplica√ß√£o:** O prompt come√ßa com "Voc√™ √© um Product Manager s√™nior especializado em transformar bugs reportados por usu√°rios em User Stories bem estruturadas." Isso estabelece a autoridade e o contexto profissional esperado nas respostas.

### 2. Few-shot Learning
**Justificativa:** Fornece exemplos concretos de entrada/sa√≠da para guiar o modelo. √â uma das t√©cnicas mais eficazes para melhorar performance, especialmente para tarefas de transforma√ß√£o de formato.

**Aplica√ß√£o:** Se√ß√£o "## EXEMPLOS" com 3 exemplos completos:
- **Exemplo 1 - Bug Simples:** Bot√£o de carrinho n√£o funciona
- **Exemplo 2 - Bug com Contexto T√©cnico:** Webhook de pagamento com HTTP 500
- **Exemplo 3 - Bug de Valida√ß√£o:** Email sem @ no cadastro

Cada exemplo mostra o bug de entrada e a User Story completa esperada (T√≠tulo, User Story Principal, Crit√©rios de Aceita√ß√£o).

### 3. Chain of Thought (CoT)
**Justificativa:** Instrui o modelo a "pensar passo a passo", melhorando o racioc√≠nio para tarefas complexas que exigem an√°lise estruturada.

**Aplica√ß√£o:** Se√ß√£o "## Workflow de An√°lise (Mental)" com 6 etapas estruturadas:
1. Leitura Completa e Exaustiva
2. Identificar o Ator
3. Identificar a Inten√ß√£o
4. Extrair o Valor
5. Classificar a Complexidade
6. Estruture os Crit√©rios de Aceite

### 4. Skeleton of Thought
**Justificativa:** Define uma estrutura de sa√≠da pr√©-determinada que garante consist√™ncia e completude das respostas, evitando omiss√µes de informa√ß√µes importantes.

**Aplica√ß√£o:** Se√ß√£o "## Formato de Sa√≠da" com duas estruturas distintas:
- **Para Bugs SIMPLES:** User Story + Crit√©rios de Aceita√ß√£o
- **Para Bugs M√âDIOS e COMPLEXOS:** Estrutura expandida com CRIT√âRIOS DE ACEITA√á√ÉO, CRIT√âRIOS T√âCNICOS, CRIT√âRIOS DE ACESSIBILIDADE, CONTEXTO DO BUG, M√âTRICAS DE SUCESSO e TASKS T√âCNICAS SUGERIDAS

---

## Resultados Finais

### Comparativo V1 vs V2

| M√©trica | V1 (Prompt Ruim) | V2 (Prompt Otimizado) | Melhoria |
|---------|------------------|----------------------|----------|
| Helpfulness | 0.45 | 0.95 | +111% |
| Correctness | 0.52 | 0.92 | +77% |
| F1-Score | 0.48 | 0.90 | +88% |
| Clarity | 0.50 | 0.96 | +92% |
| Precision | 0.46 | 0.94 | +104% |
| **M√âDIA** | **0.48** | **0.94** | **+94%** |

### Link LangSmith
**Dashboard do Projeto:** [prompt-optimization-challenge-resolved](https://smith.langchain.com/projects/prompt-optimization-challenge-resolved)

**Prompt Otimizado:** [fbordon/bug_to_user_story_v2](https://smith.langchain.com/hub/fbordon/bug_to_user_story_v2)

### Detalhes da Avalia√ß√£o

**Configura√ß√£o:**
- Provider: Google (Gemini)
- Modelo Principal: gemini-2.5-flash
- Modelo de Avalia√ß√£o: gemini-2.5-flash
- Dataset: 15 exemplos de bugs (5 simples, 7 m√©dios, 3 complexos)

**Resultado Final:**
```
M√©tricas LangSmith:
  - Helpfulness: 0.95 ‚úì
  - Correctness: 0.92 ‚úì

M√©tricas Customizadas:
  - F1-Score: 0.90 ‚úì
  - Clarity: 0.96 ‚úì
  - Precision: 0.94 ‚úì

üìä M√âDIA GERAL: 0.94 ‚úì
‚úÖ STATUS: APROVADO (m√©dia >= 0.9)
```

---

## Como Executar

### Pr√©-requisitos
- Python 3.9+
- API Key OpenAI ou Google Gemini
- Conta no LangSmith

### Passo 1: Setup
```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env
# Editar .env com suas credenciais
```

### Passo 2: Pull do Prompt Inicial
```bash
python src/pull_prompts.py
```

### Passo 3: Otimizar Prompt
Editar manualmente `prompts/bug_to_user_story_v2.yml` aplicando t√©cnicas de Prompt Engineering.

### Passo 4: Push do Prompt Otimizado
```bash
# Configure USERNAME_LANGSMITH_HUB no .env primeiro
python src/push_prompts.py
```

### Passo 5: Executar Testes
```bash
pytest tests/test_prompts.py -v
```

### Passo 6: Avaliar
```bash
python src/evaluate.py
```

### Passo 7: Iterar (se necess√°rio)
Se alguma m√©trica < 0.9, repetir Passos 3-6 at√© atingir o objetivo.

---

## Checklist de Implementa√ß√£o

- [x] `.env` configurado com todas as credenciais
- [x] `src/pull_prompts.py` implementado e testado
- [x] `prompts/bug_to_user_story_v1.yml` dispon√≠vel (via pull ou manual)
- [x] `prompts/bug_to_user_story_v2.yml` criado e otimizado
  - [x] Pelo menos 2 t√©cnicas aplicadas (Role Prompting, Few-shot, CoT)
  - [x] Exemplos few-shot inclu√≠dos (3 exemplos completos)
  - [x] Sem TODOs
  - [x] Role/Persona definida
- [x] `src/push_prompts.py` implementado e testado
- [x] `tests/test_prompts.py` com 6 testes implementados
  - [x] `test_prompt_has_system_prompt`
  - [x] `test_prompt_has_role_definition`
  - [x] `test_prompt_mentions_format`
  - [x] `test_prompt_has_few_shot_examples`
  - [x] `test_prompt_no_todos`
  - [x] `test_minimum_techniques`
- [x] Todos os testes passando (`pytest`)
- [x] README.md atualizado com documenta√ß√£o completa
